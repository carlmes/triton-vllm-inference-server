#!/bin/bash
set -e

# display usage
$STI_SCRIPTS_PATH/usage

# If the model_repository directory doesnt exist, create it
# This prevents a contain crash when a PV is mounted, but the directory 
# overlayed with an empty folder in from a brand new PV
if [ ! -d "$MODEL_REPOSITORY" ]; then
  echo "Creating a new empty model repository at: ${MODEL_REPOSITORY}"
  mkdir -p $MODEL_REPOSITORY
fi

#export TRANSFORMERS_CACHE=/models/
#export TRITON_CLOUD_CREDENTIAL_PATH="/opt/cloud_credential.json"

# https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html?highlight=aws_access_key_id#s3
# export TRITON_AWS_MOUNT_DIRECTORY=...

# ENTRYPOINT ["/opt/nvidia/nvidia_entrypoint.sh", "tritonserver", "--model-repository", "/triton-inference-server/docs/examples/model_repository"]
tritonserver \
  --model-repository=${MODEL_REPOSITORY} \
  --model-control-mode=poll \
  --repository-poll-secs=60
