#!/bin/bash
# source /opt/app-root/etc/generate_container_user

set -e

# display usage
$STI_SCRIPTS_PATH/usage

# Download NVIDIA sample models from GitHub
#cd ${MODEL_REPOSITORY:-$HOME/models} 
#git clone https://github.com/triton-inference-server/server.git triton-inference-server
#cd ${MODEL_REPOSITORY:-$HOME}/triton-inference-server/docs/examples
#./fetch_models.sh

# Download model from Huggingface: 
# https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3
cd ${MODEL_REPOSITORY:-$HOME/models} 
git lfs --version
echo "HuggingFace User: ${HF_USER}"
echo "HuggingFace Token: ${HF_TOKEN}"
if [ ! -d "Mistral-7B-Instruct-v0.3" ]; then
    echo "Model not found, downloading from HuggingFace..."
    git clone https://${HF_USER}:${HF_TOKEN}@huggingface.co/mistralai/Mistral-7B-Instruct-v0.3
    cd Mistral-7B-Instruct-v0.3
else
    echo "Model already downloaded, validating against latest in HuggingFace..."
    cd Mistral-7B-Instruct-v0.3
    git pull origin
fi
# https://github.com/triton-inference-server/server/issues/3623
mkdir 1


# ENTRYPOINT ["/opt/nvidia/nvidia_entrypoint.sh", "tritonserver", "--model-repository", "/triton-inference-server/docs/examples/model_repository"]
cd $HOME
tritonserver \
  --model-repository=${MODEL_REPOSITORY:-$HOME/models} \
  --model-control-mode=poll \
  --repository-poll-secs=60